<!-- Logo -->
<p align="center">
  <img src="https://raw.githubusercontent.com/multimind-dev/multimind-sdk/main/assets/logo.png" alt="MultiMind SDK Logo" width="120"/>
</p>

# MultiMind SDK

MultiMind SDK is a powerful and flexible library for fine-tuning and adapting large language models using Parameter-Efficient Fine-Tuning (PEFT) methods. It provides advanced features for meta-learning, few-shot learning, and transfer learning, making it easier to adapt models to new tasks with minimal data.

## Features

- **Unified PEFT Framework:** LoRA, Adapters, Prefix/Prompt Tuning, IA¬≥, and more
- **Meta-Learning:** MAML, Reptile, prototype-based few-shot learning
- **Transfer & Multi-Task Learning:** Layer transfer, dynamic task weighting, cross-task distillation
- **Adaptive Methods:** Resource-aware, performance-based, model-agnostic
- **Framework Integrations:** LangChain, CrewAI, LiteLLM, SuperAGI, and more

---

## üöÄ Installation

```bash
# Basic installation
pip install multimind-sdk

# With development dependencies
pip install multimind-sdk[dev]

# With specific framework support
pip install multimind-sdk[langchain,lite-llm,superagi]
```

---

## üõ†Ô∏è Usage Examples

### Quick Start Examples

We provide comprehensive examples demonstrating various features of the SDK. Check out the [examples directory](examples/) for:

- [Basic Agent Usage](examples/basic_agent.py) - Creating and using agents with different models
- [Prompt Chaining](examples/prompt_chain.py) - Complex reasoning with multi-step prompts
- [Task Running](examples/task_runner.py) - Orchestrating complex workflows
- [MCP Workflows](examples/mcp_workflow.py) - Model Composition Protocol examples
- [Usage Tracking](examples/usage_tracking.py) - Monitoring model usage and costs

Each example includes detailed comments and demonstrates best practices. See the [examples README](examples/README.md) for more details.

### Basic PEFT Fine-Tuning
```python
from multimind.fine_tuning import UniPELTPlusTuner
from datasets import load_dataset

dataset = load_dataset("glue", "mrpc")
tuner = UniPELTPlusTuner(
    base_model_name="bert-base-uncased",
    output_dir="./output",
    available_methods=["lora", "adapter"],
    model_type="causal_lm"
)
tuner.train(
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)
```

### Few-Shot Learning (MAML)
```python
from multimind.fine_tuning import FewShotMetaTuner
from datasets import load_dataset

dataset = load_dataset("tweet_eval", "sentiment")
tuner = FewShotMetaTuner(
    base_model_name="bert-base-uncased",
    output_dir="./output",
    tasks=["sentiment", "emotion"],
    available_methods=["lora"],
    few_shot_strategy="maml"
)
tuner.train(
    train_datasets={"sentiment": dataset["train"]},
    few_shot_tasks=["sentiment"]
)
```

### Framework Integration (LangChain)
```python
from multimind.integrations import create_adapter
from langchain.llms import LLMChain
from langchain.prompts import PromptTemplate

adapter = create_adapter(
    framework="langchain",
    model_path="./output/fine_tuned_model"
)
prompt = PromptTemplate(input_variables=["text"], template="Analyze sentiment: {text}")
chain = LLMChain(llm=adapter, prompt=prompt)
result = chain.run("I love this product!")
print(result)
```

---

## üìö Documentation

- [Full Documentation](https://multimind-sdk.readthedocs.io/)
- [API Reference](https://multimind-sdk.readthedocs.io/en/latest/api.html)
- [Architecture Overview](docs/architecture.md)
- [Development Guide](docs/development.md)

---

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.
- [Report bugs or request features](https://github.com/multimind-dev/multimind-sdk/issues)

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üì£ About

Your SDK solves all of this. One interface. Unified logic. Local + hosted models. Fine-tuning. Agent tools. No switching headaches.

[www.multimind.dev](https://www.multimind.dev)

---

## üñ•Ô∏è Command-Line Interface

MultiMind SDK provides a powerful CLI for training, evaluation, inference, and model management.

### Basic Usage

```bash
multimind train --config configs/train_config.yaml
# or (alias)
multimind finetune --config configs/train_config.yaml
```

### Example Config (YAML or JSON)

<details>
<summary>train_config.yaml</summary>

```yaml
base_model_name: "bert-base-uncased"
output_dir: "./output"
methods:
  - lora
  - adapter
method_configs:
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj"]
    lora_dropout: 0.05
    bias: "none"
  adapter:
    adapter_type: "houlsby"
    adapter_size: 64
    adapter_non_linearity: "relu"
    adapter_dropout: 0.1
    target_modules: ["k_proj", "o_proj"]
training_args:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  learning_rate: 1e-3
  fp16: true
  logging_steps: 10
  save_strategy: "epoch"
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
train_dataset: "path/to/train_dataset.json"
eval_dataset: "path/to/eval_dataset.json"
model_type: "causal_lm"
```
</details>

- You can use either YAML or JSON for the config file.
- Both `multimind train` and `multimind finetune` work identically.
- The CLI will prompt for missing arguments interactively.

### More CLI Commands

- `multimind evaluate --model ./output/model --dataset data.json`
- `multimind infer --model ./output/model --input "What is PEFT?"`
- `multimind list-models`
- `multimind download --model bert-base-uncased`
- `multimind export --model ./output/model --format onnx --output ./output/model.onnx`
- `multimind delete --model ./output/model`
- `multimind config --set default_dir ./models`
- `multimind info`
- `multimind completion bash`

See `multimind --help` for all options.

---

## üóÇÔ∏è Project Structure (Current & Planned)

Below is the planned modular structure for MultiMind SDK. **Modules marked as [implemented] are available now; others are planned for future releases.**

```
multimind-sdk/
‚îú‚îÄ‚îÄ multimind/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # [implemented] Central config loader
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # [implemented] Unified model wrappers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                # [implemented]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.py              # [implemented]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude.py              # [planned]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mistral.py             # [planned]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ huggingface.py         # [implemented]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama.py              # [implemented]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ router/                    # [implemented] Fallback, latency-aware, cost-aware logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ strategy.py            # [implemented]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fallback.py            # [implemented]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ router.py              # [implemented]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag/                       # [implemented] Retrieval-Augmented Generation support
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                # [implemented]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedder.py            # [implemented]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_store.py        # [implemented]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ fine_tuning/              # [implemented] Training logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lora_trainer.py        # [implemented]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qlora_trainer.py       # [implemented]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset_loader.py      # [implemented]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # [planned] Agent abstraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py               # [planned]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory.py              # [planned]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_loader.py        # [planned]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ web_search.py      # [planned]
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ calculator.py      # [planned]
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ file_reader.py     # [planned]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ orchestration/           # [planned] Prompt chaining and flow control
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_chain.py        # [planned]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ task_runner.py         # [planned]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ mcp/                     # [planned] Model Composition Protocol
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py             # [planned]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor.py           # [planned]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.json           # [planned]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ integrations/           # [implemented] Compatibility with LangChain, CrewAI, etc.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ langchain_adapter.py   # [implemented]
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crewai_adapter.py      # [implemented]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lite_llm.py           # [implemented]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ logging/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trace_logger.py        # [planned]
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ usage_tracker.py       # [planned]
‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                # [implemented] multimind CLI entry
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agent.py           # [planned]
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ run_mcp.py         # [planned]
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ finetune.py        # [planned]
‚îÇ
‚îú‚îÄ‚îÄ examples/
‚îú‚îÄ‚îÄ configs/                    # [implemented] Sample config files (e.g., train_config.yaml)
‚îú‚îÄ‚îÄ tests/                      # [implemented]
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ setup.py
```

> **Note:** This structure is forward-looking. All [implemented] modules are available now; [planned] modules will be added in future releases. Current functionality is not impacted.
